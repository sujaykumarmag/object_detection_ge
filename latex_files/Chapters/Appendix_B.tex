% An appendix
%======================================================================
\chapter{Python Implementation}
%======================================================================
\section{Libraries}
\begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt
import numpy as np 
import os
from patchify import patchify
import cv2
from keras.models import load_model
import random
from PIL import Image



#Imports
import os
from google.colab import drive
from PIL import Image
import os
from pathlib import Path
from google.auth.transport.requests import AuthorizedSession
from google.oauth2 import service_account
from pprint import pprint
import json
import ee
from IPython.display import Image


import matplotlib.pyplot as plt
import numpy as np 
import os
from keras.models import load_model
from keras.preprocessing.image import ImageDataGenerator
import cv2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping
import random



import cv2
import numpy as np
import os
import matplotlib.pyplot as plt
from PIL import Image
from patchify import patchify
import splitfolders
import random
from keras.utils import to_categorical




\end{lstlisting}


\section{Code} 
\subsection{Pre-Processing}
\begin{lstlisting}[language=Python]
from google.colab import drive
drive.mount('/content/drive')
img=cv2.imread("/content/drive/My Drive/Objectdet0/fyp/landcover.ai.v1/images/N-33-60-D-c-4-2.tif")
plt.figure(figsize=(18,10))
plt.subplot(131)
plt.title("R-channel")
print(img)
plt.imshow(img[:,:,0])
plt.subplot(132)

plt.title("G-channel")
plt.imshow(img[:,:,1])
plt.subplot(133)
plt.title("B-channel")
plt.imshow(img[:,:,2])
plt.show()



img=cv2.imread("/content/drive/My Drive/Objectdet0/fyp/landcover.ai.v1/masks/N-33-60-D-c-4-2.tif",0)
print(img.shape)
labels, count=np.unique(img, return_counts=True)
print(labels, count)
n_classes=len(labels)
# converting to categorical data
img=to_categorical(img, num_classes=n_classes)
plt.figure(figsize=(18,18))
for i in range(n_classes):
    plt.subplot(161+i)
    plt.title(f"Channel {i+1}")
    plt.imshow(img[:,:,i])
plt.show()


img=cv2.imread("/content/drive/My Drive/Objectdet0/fyp/landcover.ai.v1/masks/N-33-60-D-c-4-2.tif",0)
# only considereing buildings and converting rest all to unlabelled background
img[img > 1] = 0
print(img.shape)
labels, count=np.unique(img, return_counts=True)
print(labels, count)
n_classes=len(labels)
# converting to categorical data
img=to_categorical(img, num_classes=n_classes)
plt.figure(figsize=(18,18))
for i in range(n_classes):
    plt.subplot(161+i)
    plt.title(f"Channel {i+1}")
    plt.imshow(img[:,:,i])
plt.show()


root_dir = "/content/drive/My Drive/Objectdet0/fyp/landcover.ai.v1/"
patch_size = 256
img_dir = root_dir+"images/"
mask_dir = root_dir+"masks/"
# new_img_dir = root_dir+"256_patches/images/"
# new_mask_dir = root_dir+"256_patches/masks/"
new_img_dir = root_dir+"256_patches_4_classes/images/"
new_mask_dir = root_dir+"256_patches_4_classes/masks/"
try:
    os.makedirs(new_img_dir)
    os.makedirs(new_mask_dir)
except:
    print("Directory already available, so not created")
img_list = sorted(os.listdir(img_dir))
msk_list = sorted(os.listdir(mask_dir))



# the images and masks with decent amout of labels are seperated and used for training.
no_use_images=0
useful_images=0

# save the 256x256 with rules as mentioned above so that they can be used for data augumentation
# resizing will change the size of real image, so divide the image into patches of 256x256x3
for img in range(len(img_list)):
    img_name=img_list[img]
    mask_name=msk_list[img]
    print(f"Analysing {img_name} with {mask_name}")
    if img_name.endswith(".tif") and mask_name.endswith(".tif"):
        # at this point, image and mask variables contains a large sized images
        image=cv2.imread(img_dir+img_name,1)
        mask=cv2.imread(mask_dir+mask_name, 0)
        # here we crop the image so that size is near to the greatest multiple of 256
        size_x = (image.shape[1]//patch_size)*patch_size
        size_y = (image.shape[0]//patch_size)*patch_size
        # converting to pillow image
        image = Image.fromarray(image)
        mask = Image.fromarray(mask)
        # cropping from top left corner
        image = image.crop((0, 0, size_x, size_y))
        mask = mask.crop((0, 0, size_x, size_y))
        image = np.array(image)
        mask = np.array(mask)
        # converting the large image into patches
        patches_image = patchify(image, (patch_size, patch_size, 3), step=patch_size)
        patches_mask = patchify(mask, (patch_size, patch_size), step=patch_size)
        
        # save the patches to local directory
        print(patches_image.shape, patches_mask.shape)
        for i in range(patches_image.shape[0]):
            for j in range(patches_image.shape[1]):
                patch_img = patches_image[i, j, :, :]
                patch_mask = patches_mask[i, j, :, :]
                # dropping the extra part created by patchify
                patch_img = patch_img[0]
                # seggregating useful and useless images
                val, counts=np.unique(patch_mask, return_counts=True)
                # 0th index store count of unlabelled pixels
                # if unlabelled pixels are atmost 95% of total pixels, then we have atleast 5% useful pixels.
                # and also atleast 5% of the mask pixels must be of building class.
                total_pixels=counts.sum()
                count_of_unlabelled_pixel_arr = counts[np.where(val == 0)[0]]
                count_of_building_pixel_arr = counts[np.where(val == 1)[0]]
                count_of_unlabelled_pixel = 0
                count_of_building_pixel = 0
                if(len(count_of_unlabelled_pixel_arr) != 0):
                    count_of_unlabelled_pixel=count_of_unlabelled_pixel_arr[0]
                if(len(count_of_building_pixel_arr) != 0):
                    count_of_building_pixel=count_of_building_pixel_arr[0]

                if(count_of_unlabelled_pixel/total_pixels < 0.95 and 
                    count_of_building_pixel/total_pixels > 0.05):
                    # only considereing buildings and converting rest all to unlabelled background
                    # patch_mask[patch_mask > 1] = 0
                    print(f"Patch {i}-{j} {patch_img.shape}, {patch_mask.shape} generated")
                    new_path_image = os.path.join(
                        new_img_dir, 
                        r"{}".format(img_name.split(".")[0]+'patch_'+str(i)+str(j)+'.tif'))
                    new_path_mask = os.path.join(
                        new_mask_dir, 
                        r"{}".format(mask_name.split(".")[0]+'patch_'+str(i)+str(j)+'.tif'))
                    cv2.imwrite(new_path_image, patch_img)
                    cv2.imwrite(new_path_mask, patch_mask)
                    useful_images+=1
                else:
                    no_use_images+=1





num_images = len(os.listdir(new_img_dir))
img_num = random.randint(0, num_images-1)
print(f'Inspect 1 patch image mask pair out of {num_images}')
new_img_list = sorted(os.listdir(new_img_dir))
new_mask_list = sorted(os.listdir(new_mask_dir))
print(new_img_list[img_num], new_img_list[img_num])
img_for_plot = cv2.imread(new_img_dir+new_img_list[img_num], 1)
mask_for_plot =cv2.imread(new_mask_dir+new_mask_list[img_num], 0)

plt.figure(figsize=(10, 8))
plt.subplot(121)
plt.imshow(img_for_plot)
plt.title('Image')
plt.subplot(122)
plt.imshow(mask_for_plot, cmap='gray')
plt.title('Mask')
plt.show()


\end{lstlisting}


\subsection{Training}
\begin{lstlisting}[language=Python]
seed = 32
root_dir = "/content/drive/My Drive/Objectdet0/fyp/landcover.ai.v1/256_patches/"
patch_size = 256
batch_size = 16
n_classes = 1
new_img_mask_dir = root_dir+"flow_dir/"
# inside train_images folder, we mock the Imagegenerator class that we have 1 class called "images" 
# similarly for all the other three folders.
train_img_dir=new_img_mask_dir+"train_images/"
train_mask_dir=new_img_mask_dir+"train_masks/"
val_img_dir=new_img_mask_dir+"val_images/"
val_mask_dir=new_img_mask_dir+"val_masks/"

img_list = os.listdir(train_img_dir+"images/")
mask_list = os.listdir(train_mask_dir+"masks/")
val_img_list = os.listdir(val_img_dir+"images/")
val_mask_list = os.listdir(val_mask_dir+"masks/")
img_list = sorted(img_list)
mask_list = sorted(mask_list)
val_img_list = sorted(val_img_list)
val_mask_list = sorted(val_mask_list)
steps_per_epoch = len(img_list)//batch_size
val_steps_per_epoch = len(val_img_list)//batch_size



# checking
num_images = len(img_list)
num_masks = len(mask_list)
img_num = random.randint(0, num_images-1)
print(f'Inspect 1 patch image mask pair out of {num_images}')
print(train_img_dir+"images/"+img_list[img_num])
print(train_mask_dir+"masks/"+mask_list[img_num])
img_for_plot = cv2.imread(train_img_dir+"images/"+img_list[img_num], 1)
mask_for_plot = cv2.imread(train_mask_dir+"masks/"+mask_list[img_num], 0)

plt.figure(figsize=(10, 8))
plt.subplot(121)
plt.imshow(img_for_plot)
plt.title('Image')
# plt.subplot(122)#plt.imshow(mask_for_plot, cmap='gray')
# plt.title('Mask')
plt.show()

print('Max value: ', np.amax(mask_for_plot))
print('Image shape: ', mask_for_plot.shape)
print('Pixels in (256,256) img with value 1 :',np.sum(mask_for_plot==1.0))
print('Pixels in (256,256) img with value 0 :',np.sum(mask_for_plot==0.0))
print('Pixels in (256,256) img with value between 0 and 1 :',np.sum(mask_for_plot>0.0)-np.sum(mask_for_plot==1.0))



X = []
for file in img_list:
    img = cv2.imread(train_img_dir+"images/"+file, 1)
    img=img/255.0
    X.append(img)
Y=[]
for file in mask_list:
    mask =cv2.imread(train_mask_dir+"masks/"+file, 0)
    mask=mask/1.0
    Y.append(mask)
print(len(X), len(Y))
# converting to numpy array
X = np.asarray(X)
print(X.shape)
# convert masks to categorical (one hot encoded) data
Y=np.asarray(Y)
# till now, the masks are integer encoded
print(Y.shape)



X_test = []
for file in val_img_list:
    img = cv2.imread(val_img_dir+"images/"+file, 1)
    img=img/255.0
    X_test.append(img)
Y_test =[]
for file in val_mask_list:
    mask =cv2.imread(val_mask_dir+"masks/"+file, 0)
    mask=mask/1.0
    Y_test.append(mask)
print(len(X_test), len(Y_test))
# converting to numpy array
X_test = np.asarray(X_test)
print(X_test.shape)
# convert masks to categorical (one hot encoded) data
Y_test=np.asarray(Y_test)
# till now, the masks are integer encoded
print(Y_test.shape)






from keras import backend as K
from keras import Input
from keras.layers import Lambda, Dropout, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate
from keras.models import Model

# Mean IOU
def jacard_coef(y_true, y_pred):
    y_true_f=K.flatten(y_true)
    y_pred_f=K.flatten(y_pred)
    intersection=K.sum(y_true_f*y_pred_f)
    return (intersection*1.0)/(K.sum(y_true_f)+K.sum(y_pred_f)-(intersection*1.0))

def jacard_loss(y_true, y_pred):
    return 1-jacard_coef(y_true, y_pred)

IMG_WIDTH=256
IMG_HEIGHT=256
IMG_CHANNELS=3

# multi-class semantic segmentation model (0 and 1, so binary class semantic segmentation)
def unet_model(n_classes):
    """ Contraction path, encoding """
    inputs=Input((IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS))
    # the layers take floating point values. So we have to convert the integers of the pixel values to floating point
    # so we divide all image by 255
    # this is lambda function over the layer
    # inputs=Lambda(lambda x:x/255)(inputs)
    # 64 feature dimensions, kernal size, 
    # he_normal is one kind of provision of starting weights of the neural network. In the process of iteration, the weights get better.
    # this is truncated around 0, and follows gaussian distribution.
    # same padding meanss output image dimensions are same as input image
    # we apply all the conv1 on the inputs layer
    conv1=Conv2D(32,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(inputs)
    # dropping out 10% of the nodes
    conv1=Dropout(0.2)(conv1)
    conv1=Conv2D(32,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv1)
    
    # pool size=(2,2)
    conv2=MaxPooling2D((2,2))(conv1)
    conv2=Conv2D(64,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv2)
    conv2=Dropout(0.2)(conv2)
    conv2=Conv2D(64,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv2)
    
    conv3=MaxPooling2D((2,2))(conv2)
    conv3=Conv2D(128,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv3)
    conv3=Dropout(0.2)(conv3)
    conv3=Conv2D(128,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv3)
    
    conv4=MaxPooling2D((2,2))(conv3)
    conv4=Conv2D(256,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv4)
    conv4=Dropout(0.2)(conv4)
    conv4=Conv2D(256,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv4)
    
    conv5=MaxPooling2D((2,2))(conv4)
    conv5=Conv2D(512,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv5)
    conv5=Dropout(0.2)(conv5)
    conv5=Conv2D(512,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv5)

    """ Expansion path, decoding """
    upconv6=Conv2DTranspose(256,(2,2), strides=(2,2), padding="same")(conv5)
    upconv6=concatenate([upconv6, conv4])
    conv6=Conv2D(256,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(upconv6)
    conv6=Dropout(0.2)(conv6)
    conv6=Conv2D(256,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv6)

    upconv7=Conv2DTranspose(128,(2,2), strides=(2,2), padding="same")(conv6)
    upconv7=concatenate([upconv7, conv3])
    conv7=Conv2D(128,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(upconv7)
    conv7=Dropout(0.2)(conv7)
    conv7=Conv2D(128,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv7)

    upconv8=Conv2DTranspose(64,(2,2), strides=(2,2), padding="same")(conv7)
    upconv8=concatenate([upconv8, conv2])
    conv8=Conv2D(64,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(upconv8)
    conv8=Dropout(0.2)(conv8)
    conv8=Conv2D(64,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv8)

    upconv9=Conv2DTranspose(32,(2,2), strides=(2,2), padding="same")(conv8)
    upconv9=concatenate([upconv9, conv1])
    conv9=Conv2D(32,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(upconv9)
    conv9=Dropout(0.2)(conv9)
    conv9=Conv2D(32,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv9)
    
    conv9=Conv2D(16,(3,3), activation="relu", kernel_initializer="he_normal", padding="same")(conv9)
    outputs=Conv2D(n_classes,(1,1), activation="sigmoid", padding="same")(conv9)


    model=Model(inputs=[inputs], outputs=[outputs])

    return model


import keras
from keras.optimizers import Adam
model = unet_model(n_classes)
# optimizer includes the backpropagation algorithms to train the model.
# binary cross entropy is used for binary classification of true or not true situations in segmentaiton.
# optimizer tries to minimize the loss function.
# jacard_coef determined "Intersection Over Union" score
'''
loss=[
        jacard_loss,
        'binary_crossentropy'
    ],
    loss_weights=[1,1],
  metrics=[
        "accuracy", 
        jacard_coef
    ]
'''
model.compile(
    optimizer=Adam(learning_rate = 1e-3), loss=jacard_loss, 
    metrics=['accuracy',jacard_coef]
)




# to avoid overfitting of model
earlystopping = EarlyStopping(
    monitor="val_loss", 
    mode="min", patience=3, 
    restore_best_weights=True
)



# make a new folder to save model
try:
    os.makedirs(root_dir+"models")
except:
    print("Directory already available, so not created")



history = model.fit(
    X,Y,
    steps_per_epoch=steps_per_epoch, 
    epochs=20, 
    verbose=1,
    callbacks=[earlystopping],
    validation_data=(X_test, Y_test),
    validation_steps=val_steps_per_epoch
)



#plot the training and validation accuracy and loss at each epoch
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'y', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


from keras.metrics import MeanIoU
n_classes = 2
IOU_keras = MeanIoU(num_classes=n_classes)  
IOU_keras.update_state(y_pred_thresh, Y_test)
print("Mean IoU =", IOU_keras.result().numpy())




# IOU for individual class
values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)
print(values)
print("\n")
class0_IOU = values[0,0]/(values[0,0]+values[0,1])
class1_IOU = values[1,1]/(values[1,0]+values[1,1])
print("Unlabelled IOU: ", class0_IOU)
print("Buildings IOU: ", class1_IOU)




for i in range(11,30):
    fig, ax = plt.subplots(1, 3)
    ax[0].imshow(X_test[i])
    ax[1].imshow(Y_test[i], cmap="gray")
    ax[2].imshow(y_pred_thresh[i],  cmap="gray")
    fig.show()




\end{lstlisting}


\subsection{GCP Connection GE API}
\begin{lstlisting}[language=Python]

PROJECT = 'objectdet0'

!gcloud auth login --project {PROJECT}

SERVICE_ACCOUNT='sujay-gcp@objectdet0.iam.gserviceaccount.com'
KEY = 'key.json'

!gcloud iam service-accounts keys create {KEY} --iam-account {SERVICE_ACCOUNT}


from google.auth.transport.requests import AuthorizedSession
from google.oauth2 import service_account

credentials = service_account.Credentials.from_service_account_file(KEY)
scoped_credentials = credentials.with_scopes(
    ['https://www.googleapis.com/auth/cloud-platform'])

session = AuthorizedSession(scoped_credentials)

url = 'https://earthengine.googleapis.com/v1beta/projects/earthengine-public/assets/LANDSAT'

response = session.get(url)

from pprint import pprint
import json
pprint(json.loads(response.content))

import ee

# Get some new credentials since the other ones are cloud scope.
ee_creds = ee.ServiceAccountCredentials(SERVICE_ACCOUNT, KEY)
ee.Initialize(ee_creds)


coords = [
  -121.58626826832939, 
  38.059141484827485,
]
region = ee.Geometry.Point(coords)

collection = ee.ImageCollection('COPERNICUS/S2')
collection = collection.filterBounds(region)
collection = collection.filterDate('2020-04-01', '2020-09-01')
image = collection.median()


serialized = ee.serializer.encode(image)

# Make a projection to discover the scale in degrees.
proj = ee.Projection('EPSG:4326').atScale(10).getInfo()

# Get scales out of the transform.
scale_x = proj['transform'][0]
scale_y = -proj['transform'][4]




url = 'https://earthengine.googleapis.com/v1beta/projects/{}/image:computePixels'
url = url.format(PROJECT)

response = session.post(
  url=url,
  data=json.dumps({
    'expression': serialized,
    'fileFormat': 'PNG',
    'bandIds': ['B4','B3','B2'],
    'grid': {
      'dimensions': {
        'width': 256,
        'height': 256
      },
      'affineTransform': {
        'scaleX': scale_x,
        'shearX': 0,
        'translateX': coords[0],
        'shearY': 0,
        'scaleY': scale_y,
        'translateY': coords[1]
      },
      'crsCode': 'EPSG:4326',
    },
    'visualizationOptions': {'ranges': [{'min': 0, 'max': 3000}]},
  })
)

image_content = response.content


# Import the Image function from the IPython.display module. 
Image(image_content)



drive.mount('/content/drive')
path = r"/content/drive/MyDrive/model_test/images"
os.chdir(path)
with open(path+"/image.png", "wb") as img:
    img.write(image_content)




\end{lstlisting}